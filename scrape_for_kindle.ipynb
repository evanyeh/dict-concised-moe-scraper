{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 台灣教育部國語辭典簡編本 網絡爬蟲\r\n",
    "# Taiwan Ministry of Education Concise Chinese Dictionary Web Crawler/Scraper\r\n",
    "## (for Kindle dictionary)\r\n",
    "\r\n",
    "This notebook is scrapes the Ministry of Education (moe) website for the simplified/concise version of the chinese dictionary. [http://dict.concised.moe.edu.tw/](http://dict.concised.moe.edu.tw/) This scraper attempts to gather data into a .json file formatted similar to [this](https://github.com/g0v/moedict-data) popular repo. Though, the motivation is to gather data for a proper Traditional Chinese Kindle dictionary, which does not require as clean of data processing after the scraping so the format is not exactly the same as the mentioned repo's. \r\n",
    "\r\n",
    "The approach is to use the listings by stroke count to gather links to each entry and then gather at least the: title, bopomofo, pinyin, and definition. Some entries have synonyms and antonyms that are also extracted.\r\n",
    "\r\n",
    "This notebook does require some babysitting as explained in the next markdown cell."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\r\n",
    "from requests_html import HTMLSession\r\n",
    "import json\r\n",
    "import time\r\n",
    "import random\r\n",
    "import cellbell"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\eyeh2\\anaconda3\\envs\\moe-dict-concised-scraper\\lib\\site-packages\\pyglet\\media\\codecs\\wmf.py:838: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems that the moe server generates a unique 6 character 'ccd' identifier that only works for some number of requests. (idk what this is called) See stroke_main_url below where ccd=KD6FAc. I'm not sure how to get a new ccd after the current one expires, but once it does (which the program will let you know) we must manually load up [http://dict.concised.moe.edu.tw/](http://dict.concised.moe.edu.tw/), go to an entry, and copy paste the new ccd from the url into the ccd variable below and rerun the notebook.\r\n",
    "\r\n",
    "The below cell doesn't always return success even with the retries. After an unsuccessful run with a new ccd, let it cool down for a few seconds and keep trying."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "session = HTMLSession()\r\n",
    "\r\n",
    "def process_url(url, ccd):\r\n",
    "    return re.sub(r'(?<=ccd=).{6}', ccd, url)\r\n",
    "\r\n",
    "ccd = 'mZ2330'\r\n",
    "# this is the main webpage that has all the buttons for number of strokes\r\n",
    "# from clicking '筆畫索引'\r\n",
    "stroke_main_url = 'http://dict.concised.moe.edu.tw/cgi-bin/jbdic/gsweb.cgi?ccd=KD6FAc&o=e0&&sec=sec1&brwtyp=sco&brwsimpfmt=11&field_1=sco1&field_1_value=XXX&field_2=sco2&brwsortby=wqyx&active=bhbrw'\r\n",
    "stroke_main_url = process_url(stroke_main_url, ccd)\r\n",
    "\r\n",
    "tries = 0\r\n",
    "MAX_TRIES = 1\r\n",
    "while tries < MAX_TRIES:\r\n",
    "    print('Attempt {}/{}'.format(tries+1, MAX_TRIES))\r\n",
    "    stroke_main = session.get(stroke_main_url)\r\n",
    "    if len(stroke_main.html.absolute_links) > 0:\r\n",
    "        # get all 1畫, 2畫, ... links\r\n",
    "        stroke_urls_elems = stroke_main.html.find('[class=sco_bst_lv1]')\r\n",
    "        stroke_urls = [url.absolute_links.pop() for url in stroke_urls_elems]\r\n",
    "        print('Success. {} stroke urls found. (Should be exactly 33)'.format(len(stroke_urls)))\r\n",
    "        break\r\n",
    "    tries += 1\r\n",
    "    time.sleep(2*random.random())\r\n",
    "    if tries == MAX_TRIES:\r\n",
    "        print('Attempts unsuccessful. Try using new ccd session id.')\r\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Attempt 1/1\n",
      "Success. 33 stroke urls found. (Should be exactly 33)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classes to represent a dictionary entry\r\n",
    "\r\n",
    "The overall hierarchy is:\r\n",
    "DictEntry:\r\n",
    "    title: str\r\n",
    "    radical: str\r\n",
    "    non_radical_stroke_count: str\r\n",
    "    stroke_count: str\r\n",
    "    heteronyms: [\r\n",
    "        Heteronym:\r\n",
    "            bopomofo: str\r\n",
    "            pinyin: str\r\n",
    "            synonyms: str\r\n",
    "            antonyms: str\r\n",
    "            definitions: [\r\n",
    "                Definition:\r\n",
    "                    defn: str\r\n",
    "                    quote: str\r\n",
    "            ]\r\n",
    "        ]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "class Definition:\r\n",
    "    \"\"\"Lowest level class for a dictionary entry. This contains the actual\r\n",
    "    text definition for an entry. There are supposed to be more fields in this\r\n",
    "    class (quote, synonyms, antonyms) but were taken out for ease of\r\n",
    "    implementation for the Kindle. The full breakdown of the definition is TBD.\r\n",
    "    For now, this class holds all definitions and examples for an entry \r\n",
    "    (everything in the 釋義 table cell). Synonyms and antonyms belong to the\r\n",
    "    Heteronym class since the extracted text has numbers labelling which\r\n",
    "    definition it corresponds to.\r\n",
    "    \"\"\"\r\n",
    "    defn: str\r\n",
    "\r\n",
    "    def __init__(self, defn: str = None):\r\n",
    "        \"\"\"Initializes definition.\r\n",
    "        \"\"\"\r\n",
    "        self.defn = defn\r\n",
    "\r\n",
    "    def __repr__(self) -> str:\r\n",
    "        return json.dumps(self.__dict__, default=lambda o: o.__dict__, ensure_ascii=False)\r\n",
    "\r\n",
    "class Heteronym:\r\n",
    "    \"\"\"Represents all data for a pronunciation of a specific dictionary entry.\r\n",
    "    For example for a DictEntry title of 不好, there would be two heteronyms \r\n",
    "    ㄅㄨˊ　ㄏㄠˋ and ㄅㄨˋ　ㄏㄠˇ, each with its own set of definitions/quotes/\r\n",
    "    synonyms/antonyms.\r\n",
    "    \"\"\"\r\n",
    "    bopomofo: str\r\n",
    "    pinyin: str\r\n",
    "    definitions: list\r\n",
    "    synonyms: str\r\n",
    "    antonyms: str\r\n",
    "\r\n",
    "    def __init__(self):\r\n",
    "        \"\"\"Initializes empty heteronym.\r\n",
    "        \"\"\"\r\n",
    "        self.bopomofo = None\r\n",
    "        self.pinyin = None\r\n",
    "        self.definitions = []\r\n",
    "        self.synonyms = None\r\n",
    "        self.antonyms = None\r\n",
    "\r\n",
    "    def __repr__(self) -> str:\r\n",
    "        return json.dumps(self.__dict__, default=lambda o: o.__dict__, ensure_ascii=False)\r\n",
    "\r\n",
    "class DictEntry:\r\n",
    "    \"\"\"A class to represent a chinese entry in a dictionary. Each entry may\r\n",
    "    contain different heternoyms. This class was created with easy __dict__ to \r\n",
    "    json conversion in a post-processing step in mind.\r\n",
    "    \r\n",
    "    title (str): the dictionary entry title (dictionary key)\r\n",
    "    heteronyms (list): list of different Heteronym objects\r\n",
    "    radical (str): the radical of the character (only for single characters)\r\n",
    "    non_radical_stroke_count (int): (only for single characters)\r\n",
    "    stroke_count (int): (only for single characters)\r\n",
    "    \"\"\"\r\n",
    "    title: str\r\n",
    "    heteronyms: list = []\r\n",
    "    radical: str\r\n",
    "    non_radical_stroke_count: int\r\n",
    "    stroke_count: int\r\n",
    "    \r\n",
    "    def __init__(self):\r\n",
    "        \"\"\"Initializes an empty dictionary entry.\r\n",
    "        \"\"\"\r\n",
    "        self.title = None\r\n",
    "        self.heteronyms = []\r\n",
    "        self.radical = None\r\n",
    "        self.non_radical_stroke_count = None\r\n",
    "        self.stroke_count = None\r\n",
    "\r\n",
    "    def __eq__(self, other) -> bool:\r\n",
    "        \"\"\"Primarily used to lookup and append heteronyms to existing entries.\r\n",
    "        \"\"\"\r\n",
    "        if isinstance(other, DictEntry):\r\n",
    "            return self.title == other.title\r\n",
    "        return False\r\n",
    "        \r\n",
    "    def __repr__(self) -> str:\r\n",
    "        # return self as __dict__, and return sub-objects as __dict__'s too\r\n",
    "        return json.dumps(self.__dict__, default=lambda o: o.__dict__, ensure_ascii=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "# html snippet of the 'example' image used to dilineate definition and examples/quotes\r\n",
    "example_snippet = '<img src=\"/jbdic/cfont/9b51._104_0.gif\" border=\"0\" alt=\"&#x4F8B;\" class=\"fontimg\" id=\"@extrafont\" />'\r\n",
    "# html snippet used to express 一 in 注音\r\n",
    "i_snippet = '<font class=\"zinspc1\">ㄧ</font>'\r\n",
    "\r\n",
    "def parse_def_page(def_page, existing_entry: DictEntry = None) -> DictEntry:\r\n",
    "    \"\"\"Given a requests_html page of a moe dictionary entry, parse the data and\r\n",
    "    grab information like title, radical, stroke counts, synonyms, antonyms,\r\n",
    "    and definitions. An existing DictEntry may be given to append a heteronym\r\n",
    "    in place.\r\n",
    "    Inputs:\r\n",
    "        def_page (requests_html page): page of a moe dictionary entry\r\n",
    "        existing_entry (DictEntry): An existing entry for a heteronym to be \r\n",
    "            appended to\r\n",
    "    \"\"\"\r\n",
    "    # grab headers on the left column\r\n",
    "    row_headers = def_page.html.find('[class=std1]')\r\n",
    "    # grab data on the right column\r\n",
    "    row_data = def_page.html.find('[class=std2]')\r\n",
    "\r\n",
    "    entry = DictEntry()\r\n",
    "    if existing_entry:\r\n",
    "        entry = existing_entry\r\n",
    "\r\n",
    "    # this page represents a heteronym, so always create a new one\r\n",
    "    heter = Heteronym()\r\n",
    "\r\n",
    "    # process each row to build up heteronym/definition\r\n",
    "    for i in range(len(row_headers)):\r\n",
    "        header = row_headers[i]\r\n",
    "        data = row_data[i]\r\n",
    "\r\n",
    "        # process each row by looking for keywords in the header \r\n",
    "        \r\n",
    "        # look for first row by seraching for a pronunciation sound file element\r\n",
    "        if len(header.find('[id=layoutwav]')) > 0:\r\n",
    "            # get rid of enclosing brackets\r\n",
    "            entry.title = data.text[1:data.text.find('】')]\r\n",
    "\r\n",
    "            # if this is a single character, then also get radical and stroke counts\r\n",
    "            # e.g. 【不】一-3-4\r\n",
    "            if '-' in data.text:\r\n",
    "                entry.radical = data.text[data.text.find('】')+1]\r\n",
    "                dash_1 = data.text.find('-') + 1\r\n",
    "                dash_2 = data.text.find('-', dash_1)\r\n",
    "                entry.non_radical_stroke_count = int(data.text[dash_1:dash_2])\r\n",
    "                entry.stroke_count = int(data.text[dash_2+1:])\r\n",
    "        elif '注音' in header.text:\r\n",
    "            # replace weird looking | for better looking 一\r\n",
    "            data.html = data.html.replace(i_snippet, '一')\r\n",
    "            # note: use .full_text here to avoid weird \\n placed everywhere with .text\r\n",
    "            heter.bopomofo = data.full_text\r\n",
    "            # print(data.text)\r\n",
    "        elif '漢語拼音' in header.text:\r\n",
    "            # note: use .full_text here to avoid weird \\n placed everywhere with .text\r\n",
    "            heter.pinyin = data.full_text\r\n",
    "            # print(data.text)\r\n",
    "        elif '相似詞' in header.text:\r\n",
    "            heter.synonyms = data.text\r\n",
    "            # print(data.text)\r\n",
    "        elif '相反詞' in header.text:\r\n",
    "            heter.antonyms = data.text\r\n",
    "            # print(data.text)\r\n",
    "        elif '釋義' in header.text:\r\n",
    "            # replace the example '例' image html with regular text instead\r\n",
    "            EX_TAG = '例:'\r\n",
    "            data.html = data.html.replace(example_snippet, EX_TAG)            \r\n",
    "\r\n",
    "            # there is some inconsistency in the formatting of pages\r\n",
    "            # for ordered lists, sometimes they are in a <ol><li></li></ol> list,\r\n",
    "            # other times it is just a big <p> element with hardcoded <br> and \r\n",
    "            # numbers\r\n",
    "\r\n",
    "            # default is second case (big <p> with hardcoded <br> and numbers)\r\n",
    "            defn = data.text\r\n",
    "\r\n",
    "            # take care of first case (<ol></ol>)\r\n",
    "            lis = data.find('li')\r\n",
    "            if len(lis) > 0:\r\n",
    "                defns = [li.text for li in lis]\r\n",
    "                # reconstruct text in same style as second case\r\n",
    "                defn = ''\r\n",
    "                for j in range(len(defns)):\r\n",
    "                    # e.g. 2.第二解釋\\n\r\n",
    "                    defn += '{}.{}\\n'.format(str(j+1), defns[j])            \r\n",
    "\r\n",
    "            # add this newly constructed definition to heteronym\r\n",
    "            heter.definitions.append(Definition(defn))\r\n",
    "\r\n",
    "            # print(data.text)\r\n",
    "    \r\n",
    "    # add this newly constructed heteronym to existing list (likely empty)\r\n",
    "    entry.heteronyms.append(heter)\r\n",
    "    \r\n",
    "    return entry\r\n",
    "\r\n",
    "def del_none(d):\r\n",
    "    \"\"\"\r\n",
    "    Delete keys with the value ``None`` in a dictionary, recursively.\r\n",
    "\r\n",
    "    This alters the input so you may wish to ``copy`` the dict first.\r\n",
    "    \"\"\"\r\n",
    "    # For Python 3, write `list(d.items())`; `d.items()` won’t work\r\n",
    "    # For Python 2, write `d.items()`; `d.iteritems()` won’t work\r\n",
    "    for key, value in list(d.items()):\r\n",
    "        if value is None:\r\n",
    "            del d[key]\r\n",
    "        elif isinstance(value, dict):\r\n",
    "            del_none(value)\r\n",
    "    return d  # For convenience\r\n",
    "\r\n",
    "def remove_nulls(d):\r\n",
    "    return {k: v for k, v in d.items() if v is not None}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main\r\n",
    "\r\n",
    "Depth first search to avoid session timeout issues complicating continuing runs. Goes in increasing stroke count order and saves all entries for a root character before continuing to next. Saves all entries for a stroke count in a separate file.\r\n",
    "\r\n",
    "This is the function to keep an eye on for babysitting. Watch for assertion error saying it needs a new ccd. To fix:\r\n",
    "1. Go visit a dictionary entry on the moe [website](http://dict.concised.moe.edu.tw/) and copy over the ccd from the url into the ccd variable in the first cell. \r\n",
    "2. Change the starting index in range(len(stroke_urls)) in the first line of code below to restart whatever stroke count it was last working on.\r\n",
    "3. Rerun whole notebook with new ccd.\r\n",
    "\r\n",
    "Edit: found that can normally resume operation without inputting new ccd if the connection is ended by remote. i.e. pattern i noticed was could rerun for a decent amount of time if it wasn't the assertion error that I raise, in which case should update the ccd."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "source": [
    "# runs always start at this index\r\n",
    "stroke_start_ind = 11\r\n",
    "# set to true if picking up in the middle of stroke_start_ind run\r\n",
    "pickup_previous = True\r\n",
    "# index of the character index of this stroke_start_ind to run first\r\n",
    "root_char_start_ind = 268\r\n",
    "\r\n",
    "\r\n",
    "# just to keep tally of how many entries were processed each run\r\n",
    "run_entry_count = 0\r\n",
    "\r\n",
    "try:\r\n",
    "    # scrape and process in increasing total stroke count\r\n",
    "    # save entries for each stroke count into separate file\r\n",
    "    # this processing is stroke-centric to segment and continue runs since the \r\n",
    "    # session seems to expire after a certain number of requests\r\n",
    "    rng_stroke = range(stroke_start_ind, len(stroke_urls))\r\n",
    "    for stroke_ind in rng_stroke:\r\n",
    "        # holds all the DictEntry objects representing each entry in the dictionary\r\n",
    "        all_entries = []\r\n",
    "        # all_entry_titles = []\r\n",
    "\r\n",
    "        if pickup_previous:\r\n",
    "            f = open(\"entries_{}.txt\".format(stroke_ind+1), \"r\", encoding='utf-8')\r\n",
    "            all_entries = [e for e in json.load(f) if 'title' in e]\r\n",
    "            # all_entry_titles = [e['title'] for e in all_entries]\r\n",
    "        \r\n",
    "        print('processing {} / {} 筆畫 urls'.format(stroke_ind+1, len(stroke_urls)))\r\n",
    "        # get urls for all root characters of this # of strokes \r\n",
    "        # e.g. 1 stroke : 一，乙\r\n",
    "        #      2 strokes: 丁，七 ...\r\n",
    "        single_stroke_url = stroke_urls[stroke_ind]\r\n",
    "        root_char_page = session.get(single_stroke_url)\r\n",
    "        root_char_elems = root_char_page.html.find('[class=sco_bst_lv2]')\r\n",
    "\r\n",
    "        assert len(root_char_elems) > 0, \\\r\n",
    "            'NO ROOT CHARS FOUND -- session probably expired, please update ccd in first cell'\r\n",
    "    \r\n",
    "        # grab the actual urls\r\n",
    "        root_char_urls = [e.absolute_links.pop() for e in root_char_elems]\r\n",
    "        # grab the actual root character in string form\r\n",
    "        root_char_strs = [e.text for e in root_char_elems]\r\n",
    "        print(len(root_char_urls), 'root characters')\r\n",
    "        # for i in range(len(root_char_strs)):\r\n",
    "        #     print(i, root_char_strs[i])\r\n",
    "        # break\r\n",
    "\r\n",
    "        rng_root_char = range(root_char_start_ind, len(root_char_urls)) if pickup_previous else range(len(root_char_urls))\r\n",
    "        pickup_previous = False\r\n",
    "\r\n",
    "        for root_char_ind in rng_root_char:\r\n",
    "            # get urls for all entries starting with a certain root character\r\n",
    "            # e.g. for 一: 一，一把罩，一把抓，一波 ...\r\n",
    "\r\n",
    "            # start with page one (of possibly many pages of entries)\r\n",
    "            root_char_page_url = root_char_urls[root_char_ind]\r\n",
    "            is_pages_left = True\r\n",
    "            current_page = 1 # used to find url of next page\r\n",
    "\r\n",
    "            while is_pages_left:\r\n",
    "                print('{} ({}/{} root chars for {} 畫) page {}'.format(root_char_strs[root_char_ind], root_char_ind+1, len(root_char_urls), stroke_ind+1, current_page))\r\n",
    "                # print(root_char_page_url)\r\n",
    "                \r\n",
    "                entry_page = session.get(root_char_page_url)\r\n",
    "                # get all entry links, which are 'slink' classes but without titles \r\n",
    "                # (which are pagination links)\r\n",
    "                entry_elems = entry_page.html.find('[class=slink]:not([title])')\r\n",
    "\r\n",
    "                assert len(entry_elems) > 0, \\\r\n",
    "                    'NO ENTRIES FOUND -- session probably expired, please update ccd in first cell'\r\n",
    "\r\n",
    "                # for each entry, go into url and extract definition\r\n",
    "                for e in entry_elems:\r\n",
    "                    entry_url = e.absolute_links.pop()\r\n",
    "                    print('found entry {} ({}/{})'.format(e.text, root_char_ind+1, len(root_char_urls)))\r\n",
    "                    def_page = session.get(entry_url)\r\n",
    "\r\n",
    "                    assert def_page is not None, \\\r\n",
    "                        'NO ENTRIES FOUND -- session probably expired, please update ccd in first cell'\r\n",
    "                    \r\n",
    "                    to_search = DictEntry()\r\n",
    "                    to_search.title = e.text\r\n",
    "                    # see if title exists already, if so, need to add as extra heteronym\r\n",
    "                    if to_search in all_entries:\r\n",
    "                        print('adding heteronym {}'.format(e.text))\r\n",
    "                        # is a heteronym (different pronunciation e.g. 樂:ㄌㄜˋ / ㄩㄝˋ)\r\n",
    "                        # find existing entry\r\n",
    "                        existing_entry = all_entries[all_entries.index(to_search)]\r\n",
    "                        # method should update entry in place\r\n",
    "                        parse_def_page(def_page, existing_entry)\r\n",
    "                    else:\r\n",
    "                        all_entry_titles.append(e.text)\r\n",
    "                        new_entry = parse_def_page(def_page)\r\n",
    "                        all_entries.append(new_entry)\r\n",
    "                        # print(new_entry)\r\n",
    "                    # time.sleep(random.random())\r\n",
    "                    run_entry_count += 1\r\n",
    "\r\n",
    "                # get url for next page\r\n",
    "                next_page_elem = entry_page.html.find('[title=\"{}\"]'.format(current_page + 1), first=True)\r\n",
    "                if next_page_elem is None:\r\n",
    "                    is_pages_left = False\r\n",
    "                    print('No more pages, moving on to next root character')\r\n",
    "                else:\r\n",
    "                    root_char_page_url = next_page_elem.absolute_links.pop()\r\n",
    "                    current_page += 1\r\n",
    "\r\n",
    "                # save entry results after each page (~10 entries)\r\n",
    "                print('saving results...', end='')\r\n",
    "                f = open(\"entries_{}.txt\".format(stroke_ind+1), \"w\", encoding='utf-8')\r\n",
    "                # convert to json string then load back into dict while cleaning out\r\n",
    "                # None fields (like most synonyms/antonyms/radical/stroke counts)\r\n",
    "                json_string = json.dumps(all_entries, default=lambda o: o.__dict__, ensure_ascii=False)\r\n",
    "                cleaned_dic = json.loads(json_string, object_hook=remove_nulls)\r\n",
    "                f.write(json.dumps(cleaned_dic, indent=4, ensure_ascii=False))\r\n",
    "                f.close()\r\n",
    "                print('done')\r\n",
    "        #         break\r\n",
    "        #     break\r\n",
    "        # break\r\n",
    "except Exception as e:\r\n",
    "    print(e)\r\n",
    "    cellbell.ding()\r\n",
    "    print(run_entry_count, ' total entries processed this run.')\r\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing 12 / 33 筆畫 urls\n",
      "529 root characters\n",
      "猶 (269/529 root chars for 12 畫) page 1\n",
      "found entry 猶 (269/529)\n",
      "found entry 猶太屯墾區 (269/529)\n",
      "found entry 猶可 (269/529)\n",
      "found entry 猶如 (269/529)\n",
      "found entry 猶在 (269/529)\n",
      "found entry 猶存 (269/529)\n",
      "found entry 猶疑 (269/529)\n",
      "found entry 猶豫 (269/529)\n",
      "No more pages, moving on to next root character\n",
      "saving results...done\n",
      "琛 (270/529 root chars for 12 畫) page 1\n",
      "found entry 琛 (270/529)\n",
      "No more pages, moving on to next root character\n",
      "saving results...done\n",
      "琢 (271/529 root chars for 12 畫) page 1\n",
      "found entry 琢 (271/529)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-199-1e6fcb676bac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m                     \u001b[0mentry_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabsolute_links\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'found entry {} ({}/{})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot_char_ind\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_char_urls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                     \u001b[0mdef_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                     \u001b[1;32massert\u001b[0m \u001b[0mdef_page\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\moe-dict-concised-scraper\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GET'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\moe-dict-concised-scraper\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\moe-dict-concised-scraper\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\moe-dict-concised-scraper\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\moe-dict-concised-scraper\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m             )\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\moe-dict-concised-scraper\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\moe-dict-concised-scraper\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\moe-dict-concised-scraper\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\moe-dict-concised-scraper\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1377\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\moe-dict-concised-scraper\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\moe-dict-concised-scraper\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\moe-dict-concised-scraper\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tally of entries of each run just to get an idea of timeout rates\r\n",
    "453\r\n",
    "945\r\n",
    "101\r\n",
    "601\r\n",
    "287\r\n",
    "127\r\n",
    "34\r\n",
    "42\r\n",
    "118\r\n",
    "490\r\n",
    "515\r\n",
    "278\r\n",
    "1046\r\n",
    "260\r\n",
    "360\r\n",
    "1072\r\n",
    "717\r\n",
    "465\r\n",
    "1555\r\n",
    "609\r\n",
    "758"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "609"
      ]
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Post processing\r\n",
    "moved all original files to original_scraped/ and copied new version to corrected/ for whatever errors found in following steps (documented in README)\r\n",
    "\r\n",
    "load all files first"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "import json\r\n",
    "import os\r\n",
    "import re\r\n",
    "\r\n",
    "data = []\r\n",
    "\r\n",
    "for i in range(1,34):\r\n",
    "    path = 'corrected/entries_{}.txt'.format(i)\r\n",
    "    if os.path.isfile(path):\r\n",
    "        f = open(path, 'r', encoding='utf-8')\r\n",
    "        data.extend(json.load(f))\r\n",
    "        f.close()\r\n",
    "        print(path, 'added')\r\n",
    "    else:\r\n",
    "        print(path, 'not found')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "corrected/entries_1.txt added\n",
      "corrected/entries_2.txt added\n",
      "corrected/entries_3.txt added\n",
      "corrected/entries_4.txt added\n",
      "corrected/entries_5.txt added\n",
      "corrected/entries_6.txt added\n",
      "corrected/entries_7.txt added\n",
      "corrected/entries_8.txt added\n",
      "corrected/entries_9.txt added\n",
      "corrected/entries_10.txt added\n",
      "corrected/entries_11.txt added\n",
      "corrected/entries_12.txt added\n",
      "corrected/entries_13.txt added\n",
      "corrected/entries_14.txt added\n",
      "corrected/entries_15.txt added\n",
      "corrected/entries_16.txt added\n",
      "corrected/entries_17.txt added\n",
      "corrected/entries_18.txt added\n",
      "corrected/entries_19.txt added\n",
      "corrected/entries_20.txt added\n",
      "corrected/entries_21.txt added\n",
      "corrected/entries_22.txt added\n",
      "corrected/entries_23.txt added\n",
      "corrected/entries_24.txt added\n",
      "corrected/entries_25.txt added\n",
      "corrected/entries_26.txt added\n",
      "corrected/entries_27.txt added\n",
      "corrected/entries_28.txt added\n",
      "corrected/entries_29.txt added\n",
      "corrected/entries_30.txt added\n",
      "corrected/entries_31.txt added\n",
      "corrected/entries_32.txt added\n",
      "corrected/entries_33.txt added\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reorder pronunciations according to labelled 一，二，三..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# longest entry is 和 with 6 entries so rlly only need to go up to 6\r\n",
    "num_lut = {'一': 0, '二': 1, '三': 2, '四': 3, '五': 4, '六': 5, '七': 6, '八': 7, '九': 8, '十':9}\r\n",
    "\r\n",
    "hets = None\r\n",
    "\r\n",
    "try:\r\n",
    "    for i in range(len(data)):\r\n",
    "        hets = data[i]['heteronyms']\r\n",
    "        if len(hets) > 1:\r\n",
    "            new_hets = [None] * len(hets)\r\n",
    "            for j in range(len(hets)):\r\n",
    "                bpmf = hets[j]['bopomofo']\r\n",
    "                ind = num_lut[bpmf[1]]\r\n",
    "                new_hets[ind] = hets[j]\r\n",
    "            data[i]['heteronyms'] = new_hets\r\n",
    "except BaseException as e:\r\n",
    "    print(e)\r\n",
    "    print(hets)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove links to other pronunciations in each definition\r\n",
    "for example for 傍\r\n",
    "\r\n",
    "(一)ㄅㄤˋ\r\n",
    "\r\n",
    "    \"defn\": \"靠近、依附。例:依山傍水\\n(二)ㄆㄤˊ\\npán\\ng\\n\\n(三)ㄅㄤ\\nbān\\ng\"\r\n",
    "\r\n",
    "(二)ㄆㄤˊ\r\n",
    "\r\n",
    "    \"defn\": \"旁邊，通「旁」。例:「賜酒大王之前，執法在傍。」（《史記．卷一二六．滑稽傳．淳于髡傳》）\\n(一)ㄅㄤˋ\\nbàn\\ng\\n\\n(三)ㄅㄤ\\nbān\\ng\"\r\n",
    "\r\n",
    "(三)ㄅㄤ\r\n",
    "\r\n",
    "    \"defn\": \"接近某一時間點。例:傍午、傍晚\\n(一)ㄅㄤˋ\\nbàn\\ng\\n\\n(二)ㄆㄤˊ\\npán\\ng\"\r\n",
    "\r\n",
    "get rid of those (一) (二) ... in the definitions\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "txt = 0\r\n",
    "nums = list(num_lut.keys())\r\n",
    "try:\r\n",
    "    for i in range(len(data)):\r\n",
    "        txt = data[i]\r\n",
    "        hets = data[i]['heteronyms']\r\n",
    "        # if there is more than 1 pronunciation then there are those pesky links\r\n",
    "        if len(hets) > 1:\r\n",
    "            # go through each pronunciation\r\n",
    "            for j in range(len(hets)):\r\n",
    "                # always only one definition in this implementation\r\n",
    "                defn = hets[j]['definitions'][0]['defn']\r\n",
    "                # print(defn)\r\n",
    "                # search for existence of any number 1 - 10\r\n",
    "                # for n in nums:\r\n",
    "                    # tosearch = '(' + n + ')'\r\n",
    "                    # ind = defn.find(tosearch)\r\n",
    "                    # if ind != -1:\r\n",
    "                    #     # only keep up to the found link\r\n",
    "                    #     defn = defn[:ind]\r\n",
    "                    #     print(defn)\r\n",
    "                pat = r'[(][一|二|三|四|五|六|七|八|九|十][)]'\r\n",
    "                found = [m.start(0) for m in re.finditer(pat, defn)]\r\n",
    "                if len(found) > 0:\r\n",
    "                    # get index of first link to other pronunciation\r\n",
    "                    # which is the (len(hets)-1)-th to last occurence\r\n",
    "                    # e.g. if some entry has 3 heteronyms,\r\n",
    "                    #      the defn's will have links to the other 2 heteronyms\r\n",
    "                    #      at the very end of the defn, so get second to last\r\n",
    "                    # some definitions have (#) as part of their def so have to\r\n",
    "                    # do it this way\r\n",
    "                    ind = found[-(len(hets)-1)]\r\n",
    "                    print(data[i]['title'])\r\n",
    "                    defn = defn[:ind]\r\n",
    "                    # print(data[i])\r\n",
    "                data[i]['heteronyms'][j]['definitions'][0]['defn'] = defn\r\n",
    "\r\n",
    "except BaseException  as e:\r\n",
    "    print(e)\r\n",
    "    print(txt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "一行\n",
      "乜\n",
      "乜\n",
      "了\n",
      "上\n",
      "下水\n",
      "亡\n",
      "勺\n",
      "大\n",
      "大家\n",
      "大汗\n",
      "大汗\n",
      "女\n",
      "女\n",
      "子\n",
      "工夫\n",
      "不好\n",
      "不勝\n",
      "不\n",
      "予\n",
      "予\n",
      "亢\n",
      "什\n",
      "仇\n",
      "公道\n",
      "公差\n",
      "公差\n",
      "六\n",
      "六\n",
      "分子\n",
      "分子\n",
      "切口\n",
      "切口\n",
      "勾\n",
      "化\n",
      "匹\n",
      "午\n",
      "天道\n",
      "天道\n",
      "尺\n",
      "屯\n",
      "心思\n",
      "手卷\n",
      "手卷\n",
      "文\n",
      "比\n",
      "比\n",
      "氏\n",
      "爪\n",
      "爪\n",
      "父\n",
      "且\n",
      "仔\n",
      "兄弟\n",
      "出處\n",
      "出處\n",
      "功夫\n",
      "占\n",
      "卡\n",
      "句\n",
      "叨\n",
      "只\n",
      "召\n",
      "可\n",
      "外傳\n",
      "外傳\n",
      "本事\n",
      "正\n",
      "正當\n",
      "正當\n",
      "生意\n",
      "生意\n",
      "疋\n",
      "疋\n",
      "石\n",
      "任\n",
      "吃\n",
      "合\n",
      "同行\n",
      "同行\n",
      "地\n",
      "地道\n",
      "地道\n",
      "地下\n",
      "圳\n",
      "圳\n",
      "好學\n",
      "好學\n",
      "好吃\n",
      "好吃\n",
      "好事\n",
      "好玩\n",
      "好玩\n",
      "字號\n",
      "字號\n",
      "并\n",
      "扛\n",
      "扛\n",
      "朴\n",
      "朴\n",
      "汗\n",
      "汗\n",
      "百\n",
      "羊\n",
      "色\n",
      "艾\n",
      "虫\n",
      "虫\n",
      "行\n",
      "行\n",
      "衣\n",
      "亨\n",
      "亨\n",
      "伯\n",
      "估\n",
      "估\n",
      "伺\n",
      "伺\n",
      "伽\n",
      "伽\n",
      "佛\n",
      "佛\n",
      "作\n",
      "刨\n",
      "吧\n",
      "吧\n",
      "吭\n",
      "吭\n",
      "呀\n",
      "告\n",
      "囤\n",
      "囤\n",
      "囪\n",
      "囪\n",
      "坊\n",
      "完了\n",
      "完了\n",
      "弄\n",
      "弟\n",
      "彷\n",
      "彷\n",
      "忪\n",
      "忪\n",
      "抓\n",
      "折\n",
      "更\n",
      "杓\n",
      "肚\n",
      "肚\n",
      "肚子\n",
      "見\n",
      "角\n",
      "谷\n",
      "足\n",
      "身\n",
      "車\n",
      "那\n",
      "那般\n",
      "那般\n",
      "那裡\n",
      "那些\n",
      "那些\n",
      "那樣\n",
      "邪\n",
      "乖乖\n",
      "些\n",
      "兒\n",
      "兒\n",
      "其\n",
      "卒\n",
      "卷\n",
      "呢\n",
      "呱\n",
      "呱\n",
      "呵\n",
      "呼號\n",
      "呼號\n",
      "和\n",
      "和\n",
      "和\n",
      "和\n",
      "咎\n",
      "奇\n",
      "妻\n",
      "妻\n",
      "妻子\n",
      "妻子\n",
      "姊\n",
      "姊\n",
      "姑娘\n",
      "姑娘\n",
      "委\n",
      "宓\n",
      "宓\n",
      "宛\n",
      "底\n",
      "怔\n",
      "怔\n",
      "所長\n",
      "所長\n",
      "抵\n",
      "拂\n",
      "拈\n",
      "拈\n",
      "拓\n",
      "拓\n",
      "拗\n",
      "拗\n",
      "放\n",
      "於\n",
      "於\n",
      "東西\n",
      "枕\n",
      "枝\n",
      "氓\n",
      "氓\n",
      "沮\n",
      "泄\n",
      "泌\n",
      "法\n",
      "泡泡\n",
      "泡泡\n",
      "的\n",
      "的\n",
      "的\n",
      "知\n",
      "空檔\n",
      "舍\n",
      "芾\n",
      "芾\n",
      "虎\n",
      "阿\n",
      "青\n",
      "亟\n",
      "亟\n",
      "便\n",
      "便當\n",
      "便當\n",
      "便宜\n",
      "俛\n",
      "俛\n",
      "俟\n",
      "俟\n",
      "冒\n",
      "剌\n",
      "南\n",
      "咯\n",
      "咯\n",
      "咯\n",
      "咱\n",
      "咳\n",
      "咳\n",
      "咻\n",
      "咽\n",
      "咽\n",
      "哀樂\n",
      "哀樂\n",
      "哄\n",
      "哇\n",
      "哇\n",
      "哈\n",
      "契\n",
      "姥\n",
      "姥\n",
      "孩兒\n",
      "屏\n",
      "度量\n",
      "度量\n",
      "待\n",
      "思\n",
      "恫\n",
      "恫\n",
      "扁\n",
      "括\n",
      "拾\n",
      "挑空\n",
      "挑空\n",
      "施\n",
      "施\n",
      "施\n",
      "星星\n",
      "枸\n",
      "枸\n",
      "枸\n",
      "查\n",
      "歪\n",
      "洒\n",
      "洒\n",
      "洗\n",
      "洞\n",
      "洩\n",
      "炮\n",
      "炸\n",
      "為\n",
      "甚\n",
      "相親\n",
      "相親\n",
      "相稱\n",
      "相稱\n",
      "省事\n",
      "省事\n",
      "看\n",
      "祇\n",
      "祇\n",
      "紅\n",
      "背\n",
      "胖\n",
      "胖\n",
      "若\n",
      "茄\n",
      "虺\n",
      "虺\n",
      "計量\n",
      "計量\n",
      "酊\n",
      "酊\n",
      "重犯\n",
      "重犯\n",
      "降服\n",
      "限量\n",
      "限量\n",
      "革\n",
      "風頭\n",
      "風\n",
      "食\n",
      "食\n",
      "倆\n",
      "個\n",
      "們\n",
      "們\n",
      "倔\n",
      "倔\n",
      "倘\n",
      "倘\n",
      "倡\n",
      "員\n",
      "哦\n",
      "哦\n",
      "哩\n",
      "哩\n",
      "哩\n",
      "哪\n",
      "埋\n",
      "夏\n",
      "娜\n",
      "娜\n",
      "娩\n",
      "娩\n",
      "孫\n",
      "孫子\n",
      "家\n",
      "射\n",
      "射\n",
      "差\n",
      "悖\n",
      "悖\n",
      "捎\n",
      "捎\n",
      "旁\n",
      "晃\n",
      "柴\n",
      "栖\n",
      "栖\n",
      "桔\n",
      "桔\n",
      "殷\n",
      "畜\n",
      "祕\n",
      "秘\n",
      "秘\n",
      "秤\n",
      "脈\n",
      "般\n",
      "般\n",
      "茲\n",
      "茲\n",
      "衰\n",
      "釘\n",
      "釘\n",
      "骨\n",
      "骨\n",
      "高中\n",
      "高中\n",
      "鬲\n",
      "鬲\n",
      "假\n",
      "偈\n",
      "副\n",
      "勒\n",
      "匙\n",
      "匙\n",
      "參\n",
      "參\n",
      "啊\n",
      "啊\n",
      "啞\n",
      "啦\n",
      "啦\n",
      "圈\n",
      "埤\n",
      "埤\n",
      "培\n",
      "培\n",
      "婁\n",
      "宿\n",
      "宿\n",
      "將\n",
      "尉\n",
      "崗\n",
      "強辯\n",
      "強辯\n",
      "強記\n",
      "強記\n",
      "得\n",
      "得\n",
      "得了\n",
      "從\n",
      "從\n",
      "徠\n",
      "徠\n",
      "悉數\n",
      "悉數\n",
      "掃\n",
      "掄\n",
      "掄\n",
      "排頭\n",
      "排頭\n",
      "排\n",
      "掖\n",
      "掙\n",
      "掙\n",
      "教\n",
      "斜\n",
      "斜\n",
      "旋\n",
      "棄養\n",
      "棄養\n",
      "欸\n",
      "欸\n",
      "殺\n",
      "涼\n",
      "混\n",
      "率\n",
      "脯\n",
      "荷\n",
      "荼\n",
      "莊家\n",
      "莎\n",
      "莎\n",
      "莘\n",
      "莘\n",
      "莞\n",
      "莞\n",
      "莞\n",
      "蚵\n",
      "蚵\n",
      "蛇\n",
      "蛇\n",
      "趿\n",
      "趿\n",
      "逢\n",
      "陶\n",
      "陸\n",
      "鳥\n",
      "鳥\n",
      "傀\n",
      "傍\n",
      "傍\n",
      "傍\n",
      "創\n",
      "勞\n",
      "喁\n",
      "喁\n",
      "喔\n",
      "喔\n",
      "喝\n",
      "喪\n",
      "喪\n",
      "單\n",
      "嵌\n",
      "嵌\n",
      "悶\n",
      "悶氣\n",
      "悶氣\n",
      "惡\n",
      "提\n",
      "散光\n",
      "散光\n",
      "敦\n",
      "景\n",
      "曾\n",
      "棋\n",
      "棋\n",
      "棲\n",
      "棲\n",
      "棹\n",
      "棹\n",
      "渦\n",
      "渴\n",
      "湛\n",
      "湯\n",
      "無\n",
      "番\n",
      "畬\n",
      "畬\n",
      "發悶\n",
      "發悶\n",
      "等分\n",
      "等分\n",
      "答\n",
      "粥\n",
      "粥\n",
      "結\n",
      "結實\n",
      "絡\n",
      "菟\n",
      "菟\n",
      "華\n",
      "菲\n",
      "著\n",
      "蛤\n",
      "蛤\n",
      "覃\n",
      "跑\n",
      "逮\n",
      "逮\n",
      "酢\n",
      "雋\n",
      "雋\n",
      "飲\n",
      "馮\n",
      "傾倒\n",
      "勦\n",
      "勦\n",
      "嗎\n",
      "嗎\n",
      "幹\n",
      "意思\n",
      "慊\n",
      "暈\n",
      "暖\n",
      "暖暖\n",
      "暖暖\n",
      "會\n",
      "會\n",
      "會\n",
      "楞\n",
      "準頭\n",
      "準頭\n",
      "溺\n",
      "滑\n",
      "當\n",
      "當頭\n",
      "當頭\n",
      "稟\n",
      "署\n",
      "萬難\n",
      "萬難\n",
      "落\n",
      "落下\n",
      "落下\n",
      "葉\n",
      "葛\n",
      "葛\n",
      "葷\n",
      "蛾\n",
      "蛾\n",
      "貉\n",
      "貉\n",
      "賈\n",
      "載\n",
      "辟邪\n",
      "辟邪\n",
      "運氣\n",
      "運氣\n",
      "鈷\n",
      "鈷\n",
      "頓\n",
      "馱\n",
      "僮\n",
      "僮\n",
      "嘔\n",
      "list index out of range\n",
      "{'title': '嘔', 'heteronyms': [{'bopomofo': '(一)ㄡˇ', 'pinyin': '(一)ǒu', 'definitions': [{'defn': '吐。例:作嘔、嘔血、嘔心瀝血\\n'}]}, {'bopomofo': '(二)ㄡ', 'pinyin': '(二)ōu', 'definitions': [{'defn': '1.歌唱。同「謳」。例:歌嘔\\n2.狀聲詞。形容小兒學語聲、鳥鳴聲、搖櫓聲、雜樂聲等。例:嘔啞、嘔呀\\n3.(一)之又音。\\n'}]}, {'bopomofo': '(三)ㄡˋ', 'pinyin': '(三)òu', 'definitions': [{'defn': '1.用言語招惹、引人生氣。例:你說這話是存心嘔我的吧！\\n2.鬱悶。例:他丟了東西已經夠嘔的了，大家就不要再責備他了！\\n'}]}], 'radical': '口', 'non_radical_stroke_count': 11, 'stroke_count': 14}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save post-processed file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "f = open('dict_concised.json', \"w\", encoding='utf-8')\r\n",
    "f.write(json.dumps(data, indent=4, ensure_ascii=False))\r\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31679a4eb095735c07bd1571adf011b0f1ce4736e1a9b18e129026e456b86c63"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('moe-dict-concised-scraper': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}